{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from gensim.corpora import WikiCorpus\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia 데이터는 pre-trained 모델 사용(메모리, 스토리지 문제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = open('../data/wiki_eng.data', 'w')\n",
    "# wiki = WikiCorpus('../data/enwiki-latest-pages-articles.xml.bz2')\n",
    "# i = 0\n",
    "# for text in tqdm(wiki.get_texts()) :\n",
    "#     output.write(bytes(' '.join(text), 'utf-8').decode('utf-8') + '\\n')\n",
    "#     i = i + 1\n",
    "# output.close()\n",
    "# print('Processing complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMS_Spam_Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sms(s):\n",
    "    # only include alphanumerics\n",
    "    s = re.sub(r\"[^A-Za-z0-9(),.!?\\'\\`&]\", \" \", s)\n",
    "    \n",
    "    # insert spaces in special characters\n",
    "    s = re.sub(r\",\", \" , \", s)\n",
    "    s = re.sub(r\"!\", \" ! \", s)\n",
    "    s = re.sub(r\"\\(\", \" \\( \", s)\n",
    "    s = re.sub(r\"\\)\", \" \\) \", s)\n",
    "    s = re.sub(r\"\\?\", \" \\? \", s)\n",
    "    s = re.sub(r\"\\&\", \" \\& \", s)\n",
    "    \n",
    "    # only include alphanumerics again\n",
    "    s = re.sub(r\"[^A-Za-z0-9(),.!?\\'\\`&]\", \" \", s)\n",
    "    \n",
    "    # reduce multiple spaces to single spaces\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
    "    \n",
    "    # reduce multiple dots to single dot\n",
    "    s = re.sub(r\"\\.{2,}\", \".\", s)\n",
    "    s = re.sub(r\"\\.\", \" . \", s)\n",
    "    return s.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_data = pd.read_csv('../data/raw/sms-spam.csv', encoding = 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_text = sms_data.v2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_text_cleaned = np.vectorize(clean_sms)(sms_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'go until jurong point , crazy .  available only in bugis n great world la e buffet .  cine there got amore wat .',\n",
       "       'ok lar .  joking wif u oni .',\n",
       "       \"free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 .  text fa to 87121 to receive entry question ( std txt rate ) t & c's apply 08452810075over18's\",\n",
       "       ..., 'pity , was in mood for that .  so . any other suggestions ?',\n",
       "       \"the guy did some bitching but i acted like i'd be interested in buying something else next week and he gave it to us for free\",\n",
       "       'rofl .  its true to its name'], \n",
       "      dtype='<U945')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_text_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/eng/sms.pickle', 'wb') as f :\n",
    "    pickle.dump(sms_text_cleaned, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sms_data, sms_text, sms_text_cleaned, clean_sms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data = pd.read_csv('../data/raw/sentiment140.csv', encoding = 'ISO-8859-1', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_text = twitter_data[5].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_twits(s):\n",
    "    # del url and tag(@)\n",
    "    s = re.sub(r'http\\S+', '', s)\n",
    "    s = re.sub('\\@\\S+', '', s)\n",
    "    \n",
    "    # only include alphanumerics\n",
    "    s = re.sub(r\"[^A-Za-z0-9(),.!?;:\\'\\`&]\", \" \", s)   # added ; and : because of emoji\n",
    "    \n",
    "    # insert spaces in special characters\n",
    "    s = re.sub(r\",\", \" , \", s)\n",
    "    s = re.sub(r\"!\", \" ! \", s)\n",
    "    s = re.sub(r\"\\?\", \" \\? \", s)\n",
    "    s = re.sub(r\"\\&\", \" \\& \", s)\n",
    "    \n",
    "    # only include alphanumerics again\n",
    "    s = re.sub(r\"[^A-Za-z0-9(),.!?;:\\'\\`&]\", \" \", s) \n",
    "    \n",
    "    # reduce multiple spaces to single spaces\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
    "    \n",
    "    # reduce multiple dots to single dot\n",
    "    s = re.sub(r\"\\.{2,}\", \".\", s)\n",
    "    s = re.sub(r\"\\.\", \" . \", s)\n",
    "    return s.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_text_cleaned = np.vectorize(clean_twits)(twitter_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ \"awww , that's a bummer .  you shoulda got david carr of third day to do it .  ;d\",\n",
       "       \"is upset that he can't update his facebook by texting it .  and might cry as a result school today also .  blah !\",\n",
       "       'i dived many times for the ball .  managed to save 50 the rest go out of bounds',\n",
       "       ..., 'are you ready for your mojo makeover ? ask me for details',\n",
       "       'happy 38th birthday to my boo of alll time ! ! ! tupac amaru shakur',\n",
       "       'happy charitytuesday'], \n",
       "      dtype='<U322')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_text_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/eng/twitter.pickle', 'wb') as f :\n",
    "    pickle.dump(twitter_text_cleaned, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "del twitter_data, twitter_text, twitter_text_cleaned, clean_twits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어 Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = WikiCorpus('../data/kor/kowiki-20181101-pages-articles.xml.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "288672it [04:17, 1121.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = open('../data/kor/wiki_kor.data', 'w')\n",
    "i = 0\n",
    "for text in tqdm(wiki.get_texts()) :\n",
    "    output.write(bytes(' '.join(text), 'utf-8').decode('utf-8') + '\\n')\n",
    "    i = i + 1\n",
    "output.close()\n",
    "print('Processing complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "del wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naver Kin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kin_data = pd.read_csv('../data/raw/raw_kin_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kin_text = kin_data.text.values.astype(np.str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_kin(s) :\n",
    "    # del url\n",
    "    s = re.sub(r'http\\S+', '', s)\n",
    "    \n",
    "    # only include alphanumerics\n",
    "    s = re.sub(r\"[^A-Za-z0-9가-힣(),.!?\\'\\`&]\", \" \", s)\n",
    "    \n",
    "    # insert spaces in special characters\n",
    "    s = re.sub(r\",\", \" , \", s)\n",
    "    s = re.sub(r\"!\", \" ! \", s)\n",
    "    s = re.sub(r\"\\?\", \" \\? \", s)\n",
    "    s = re.sub(r\"\\&\", \" \\& \", s)\n",
    "        \n",
    "    # replace \\xa0 tag\n",
    "    s = re.sub(r\"\\xa0\", \" \", s)\n",
    "    \n",
    "    # only include alphanumerics again\n",
    "    s = re.sub(r\"[^A-Za-z0-9가-힣(),.!?\\'\\`&]\", \" \", s)\n",
    "    \n",
    "    # reduce multiple spaces to single spaces\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
    "    \n",
    "    # reduce multiple emotional character to single caracter\n",
    "    s = re.sub(r\"ㅋ{2,}\", \" \", s)\n",
    "    s = re.sub(r\"ㅎ{2,}\", \" \", s)\n",
    "    s = re.sub(r\"ㅜ{2,}\", \" \", s)\n",
    "    s = re.sub(r\"ㅠ{2,}\", \" \", s)\n",
    "    \n",
    "    # reduce multiple dots to single dot\n",
    "    s = re.sub(r\"\\.{2,}\", \".\", s)\n",
    "    s = re.sub(r\"\\.\", \" . \", s)\n",
    "    \n",
    "    return s.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_slicer(text) :\n",
    "    result = list()\n",
    "    splited_text = [a + '다.' for a in text.split('다.')]\n",
    "    if len(splited_text) > 1 :\n",
    "        del splited_text[-1]\n",
    "    else :\n",
    "        splited_text[0] = splited_text[0][:-2]\n",
    "    for sentence in splited_text : \n",
    "        if '요.' in sentence :\n",
    "            splited_sentence = [a + '요.' for a in sentence.split('요.')]\n",
    "            splited_sentence[-1] = splited_sentence[-1][:-2]\n",
    "            result.extend(splited_sentence)\n",
    "        else :\n",
    "            result.extend([sentence])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kin_text_cleaned = list()\n",
    "for sentence in kin_text :\n",
    "    kin_text_cleaned.extend(sentence_slicer(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/kor/naver_kin.data', 'w') as f :\n",
    "    for sentence in kin_text_cleaned :\n",
    "        f.write(sentence + '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del kin_data, kin_text, kin_text_cleaned, clean_kin, sentence_slicer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naver movie reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_movie(s) :\n",
    "    # del url\n",
    "    s = re.sub(r'http\\S+', '', s)\n",
    "    \n",
    "    # only include alphanumerics\n",
    "    s = re.sub(r\"[^A-Za-z0-9가-힣(),.!?\\'\\`&]\", \" \", s)\n",
    "    \n",
    "    # insert spaces in special characters\n",
    "    s = re.sub(r\",\", \" , \", s)\n",
    "    s = re.sub(r\"!\", \" ! \", s)\n",
    "    s = re.sub(r\"\\?\", \" \\? \", s)\n",
    "    s = re.sub(r\"\\&\", \" \\& \", s)\n",
    "\n",
    "    # only include alphanumerics again\n",
    "    s = re.sub(r\"[^A-Za-z0-9가-힣(),.!?\\'\\`&]\", \" \", s)\n",
    "    \n",
    "    # reduce multiple spaces to single spaces\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
    "    \n",
    "    # reduce multiple emotional character to single caracter\n",
    "    s = re.sub(r\"ㅋ{2,}\", \" \", s)\n",
    "    s = re.sub(r\"ㅎ{2,}\", \" \", s)\n",
    "    s = re.sub(r\"ㅜ{2,}\", \" \", s)\n",
    "    s = re.sub(r\"ㅠ{2,}\", \" \", s)\n",
    "    \n",
    "    # reduce multiple dots to single dot\n",
    "    s = re.sub(r\"\\.{2,}\", \".\", s)\n",
    "    s = re.sub(r\"\\.\", \" . \", s)\n",
    "    \n",
    "    return s.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_data = pd.read_csv('../data/raw/naver_movie.txt', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_text = reply_data.document.astype(np.str).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_text_cleaned = np.vectorize(clean_movie)(reply_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/kor/movie.pickle', 'wb') as f :\n",
    "    pickle.dump(reply_text_cleaned, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "del reply_data, reply_text, reply_text_cleaned, clean_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
