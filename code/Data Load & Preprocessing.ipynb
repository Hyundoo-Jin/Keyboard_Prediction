{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from gensim.corpora import WikiCorpus\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex\n",
    "import lxml.etree as ET\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikinews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_wiki(text):\n",
    "    text = regex.sub(\"\\[http[^]]+? ([^]]+)]\", r\"\\1\", text) \n",
    "    text = regex.sub(\"\\[http[^]]+]\", \"\", text) \n",
    "    text = regex.sub(\"(?s)<ref>.+?</ref>\", \"\", text) # remove reference links\n",
    "    text = regex.sub(\"(?s)<[^>]+>\", \"\", text) # remove html tags\n",
    "    text = regex.sub(\"&[a-z]+;\", \"\", text) # remove html entities\n",
    "    text = regex.sub(\"(?s){{.+?}}\", \"\", text) # remove markup tags\n",
    "    text = regex.sub(\"(?s){.+?}\", \"\", text) # remove markup tags\n",
    "    text = regex.sub(\"(?s)\\[\\[([^]]+\\|)\", \"\", text) # remove link target strings\n",
    "    text = regex.sub(\"(?s)\\[\\[([^]]+\\:.+?]])\", \"\", text) # remove media links\n",
    "    \n",
    "    text = regex.sub(\"[']{5}\", \"\", text) # remove italic+bold symbols\n",
    "    text = regex.sub(\"[']{3}\", \"\", text) # remove bold symbols\n",
    "    text = regex.sub(\"[']{2}\", \"\", text) # remove italic symbols\n",
    "    \n",
    "    text = regex.sub(u\"[^ \\r\\n\\p{Latin}\\d\\-'.?!]\", \" \", text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = regex.sub(\"[ ]{2,}\", \" \", text) # Squeeze spaces.\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_adder(sentence) :\n",
    "    return '<s> ' + sentence +  ' </s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:56<00:00, 17.69s/it]\n"
     ]
    }
   ],
   "source": [
    "result_text = list()\n",
    "fs = glob.glob('../data/raw/wikinews/*.xml')\n",
    "ns = \"{http://www.mediawiki.org/xml/export-0.10/}\" # namespace\n",
    "for f in tqdm(fs):\n",
    "    for _, elem in ET.iterparse(f, tag=ns+\"text\"):\n",
    "        try:\n",
    "            running_text = elem.text\n",
    "            running_text = running_text.split(\"===\")[0]\n",
    "            running_text = clean_wiki(running_text)\n",
    "            paras = running_text.split(\"\\n\")\n",
    "            for para in paras:\n",
    "                if len(para) > 200 :\n",
    "                    sents = [regex.sub(\"([.!?]+$)\", \"\", sent) for sent in sent_tokenize(para.strip())]\n",
    "                    result_text.extend(list(map(token_adder, sents)))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        elem.clear() # We need to save memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del f, fs, elem, running_text, paras, para, sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(text, maximum_length):\n",
    "    from collections import Counter\n",
    "    \n",
    "    unique = Counter(text)\n",
    "    orders = unique.most_common()\n",
    "    count = [['<unk>', -1], ['<pad>', -1]]\n",
    "    count.extend(orders)\n",
    "    \n",
    "    dictionary = {}\n",
    "    for word, _ in count :\n",
    "        dictionary[word] = len(dictionary)\n",
    "        \n",
    "    data = []\n",
    "    seq_lengths = []\n",
    "    sentence_data = []\n",
    "    for word in tqdm(text) :\n",
    "        if word in dictionary :\n",
    "            index = dictionary[word]\n",
    "        else :\n",
    "            index = 0\n",
    "            count[0][1] += 1\n",
    "        sentence_data.append(index)\n",
    "        if word == '</s>' :\n",
    "            if len(sentence_data) > maximum_length :\n",
    "                sentence_data = []\n",
    "            else :\n",
    "                seq_lengths.append(len(sentence_data))\n",
    "                sentence_data.extend([1] * (maximum_length - len(sentence_data)))\n",
    "                data.append(sentence_data)\n",
    "                sentence_data = []\n",
    "                \n",
    "    return np.array(data), np.array(seq_lengths), count, list(dictionary.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80734990/80734990 [00:39<00:00, 2040692.14it/s]\n"
     ]
    }
   ],
   "source": [
    "data, seq_lengths, count, ordered_words = build_dataset(' '.join(result_text).split(), 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = random.sample(range(len(data) -1), int(len(data) * 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = np.delete(np.arange(len(data)), train_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/wikinews/word_vocab.pkl', 'wb') as f :\n",
    "    pickle.dump(ordered_words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/wikinews/wiki_train.pkl', 'wb') as f :\n",
    "    pickle.dump([data[train_index], seq_lengths[train_index]], f)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/wikinews/wiki_test.pkl', 'wb') as f :\n",
    "    pickle.dump([data[test_index], seq_lengths[test_index]], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data, seq_lengths, count, ordered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyboard prediction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/wordprediction/en-sents-shuf.00.train.txt', 'r') as f :\n",
    "    temp_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/wordprediction/en-sents-shuf.00.valid.txt', 'r') as f :\n",
    "    temp_data.extend(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/wordprediction/en-sents-shuf.00.test.txt', 'r') as f :\n",
    "    temp_data.extend(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193939466/193939466 [01:42<00:00, 1890862.44it/s]\n"
     ]
    }
   ],
   "source": [
    "data, seq_lengths, count, ordered_words = build_dataset(' '.join(temp_data).split(), 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = random.sample(range(len(data) -1), int(len(data) * 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = np.delete(np.arange(len(data)), train_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/wordprediction/word_vocab.pkl', 'wb') as f :\n",
    "    pickle.dump(ordered_words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/wordprediction/wordprediction_train.pkl', 'wb') as f :\n",
    "    pickle.dump([data[train_index], seq_lengths[train_index]], f)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/wordprediction/wordprediction_test.pkl', 'wb') as f :\n",
    "    pickle.dump([data[test_index], seq_lengths[test_index]], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data, seq_lengths, count, ordered_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
