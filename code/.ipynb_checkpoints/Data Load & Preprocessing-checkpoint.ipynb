{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from gensim.corpora import WikiCorpus\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex\n",
    "import lxml.etree as ET\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikinews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = regex.sub(\"\\[http[^]]+? ([^]]+)]\", r\"\\1\", text) \n",
    "    text = regex.sub(\"\\[http[^]]+]\", \"\", text) \n",
    "    text = regex.sub(\"(?s)<ref>.+?</ref>\", \"\", text) # remove reference links\n",
    "    text = regex.sub(\"(?s)<[^>]+>\", \"\", text) # remove html tags\n",
    "    text = regex.sub(\"&[a-z]+;\", \"\", text) # remove html entities\n",
    "    text = regex.sub(\"(?s){{.+?}}\", \"\", text) # remove markup tags\n",
    "    text = regex.sub(\"(?s){.+?}\", \"\", text) # remove markup tags\n",
    "    text = regex.sub(\"(?s)\\[\\[([^]]+\\|)\", \"\", text) # remove link target strings\n",
    "    text = regex.sub(\"(?s)\\[\\[([^]]+\\:.+?]])\", \"\", text) # remove media links\n",
    "    \n",
    "    text = regex.sub(\"[']{5}\", \"\", text) # remove italic+bold symbols\n",
    "    text = regex.sub(\"[']{3}\", \"\", text) # remove bold symbols\n",
    "    text = regex.sub(\"[']{2}\", \"\", text) # remove italic symbols\n",
    "    \n",
    "    text = regex.sub(u\"[^ \\r\\n\\p{Latin}\\d\\-'.?!]\", \" \", text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = regex.sub(\"[ ]{2,}\", \" \", text) # Squeeze spaces.\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_adder(sentence) :\n",
    "    return '<s> ' + sentence +  ' </s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:25<00:00, 19.75s/it]\n"
     ]
    }
   ],
   "source": [
    "result_text = list()\n",
    "fs = glob.glob('../data/raw/wikinews/*.xml')\n",
    "ns = \"{http://www.mediawiki.org/xml/export-0.10/}\" # namespace\n",
    "for f in tqdm(fs):\n",
    "    for _, elem in ET.iterparse(f, tag=ns+\"text\"):\n",
    "        try:\n",
    "            running_text = elem.text\n",
    "            running_text = running_text.split(\"===\")[0]\n",
    "            running_text = clean_text(running_text)\n",
    "            paras = running_text.split(\"\\n\")\n",
    "            for para in paras:\n",
    "                if len(para) > 200 :\n",
    "                    sents = [regex.sub(\"([.!?]+$)\", \"\", sent) for sent in sent_tokenize(para.strip())]\n",
    "                    result_text.extend(list(map(token_adder, sents)))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        elem.clear() # We need to save memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "del f, fs, elem, running_text, paras, para, sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_text = np.array(result_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = random.sample(range(len(result_text) -1), int(len(result_text) * 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_vocab(text):\n",
    "    from collections import Counter\n",
    "    from itertools import chain\n",
    "    \n",
    "    word2cnt = Counter(' '.join(text).split())\n",
    "    vocab = [\"<EMP>\", \"<UNK>\"] + [word for word, cnt in word2cnt.items() if cnt > 50]\n",
    "    word2idx = {word:idx for idx, word in enumerate(vocab)}\n",
    "    idx2word = {idx:word for idx, word in enumerate(vocab)} \n",
    "    pickle.dump( (word2idx, idx2word), open(\"../data/wikinews/word_vocab.pkl\", \"wb\") )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_word_vocab(result_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = np.delete(np.arange(len(result_text)), train_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/wikinews/wiki_train.txt', 'w') as f :\n",
    "    for sentence in result_text[train_index] :\n",
    "        f.write(sentence + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/wikinews/wiki_test.txt', 'w') as f :\n",
    "    for sentence in result_text[test_index] :\n",
    "        f.write(sentence + '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "del result_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyboard prediction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/unked-clean-dict-15k/en-sents-shuf.00.test.txt', 'r') as f :\n",
    "    test_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/unked-clean-dict-15k/en-sents-shuf.00.train.txt', 'r') as f :\n",
    "    train_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/unked-clean-dict-15k/en-sents-shuf.00.valid.txt', 'r') as f :\n",
    "    valid_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter()\n",
    "for sentence in [train_data, test_data, valid_data] :\n",
    "    c.update(' '.join(sentence).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"<EMP>\", \"<UNK>\"] + [word for word, cnt in c.items() if cnt > 50]\n",
    "word2idx = {word:idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx:word for idx, word in enumerate(vocab)} \n",
    "pickle.dump( (word2idx, idx2word), open(\"../data/unked-clean-dict-15k/keyboard_vocab.pkl\", \"wb\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "del c, sentence, test_data, train_data, valid_data, vocab, word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
